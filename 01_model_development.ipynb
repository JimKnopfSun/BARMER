{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"' \n",
    "     'frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Database built from the recordings of 30 subjects performing activities of daily living while carrying a waist-mounted smartphone with embedded inertial sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Project: https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Connect to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client_hdfs = InsecureClient('http://awscdh6-ma.sap.local:9870', user='dr.who')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_labeled_performance',\n",
       " 'data_labeled_training',\n",
       " 'data_unlabeled_predictions',\n",
       " 'model']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Data (labeled) from Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body_acc_x.txt',\n",
       " 'body_acc_y.txt',\n",
       " 'body_acc_z.txt',\n",
       " 'body_gyro_x.txt',\n",
       " 'body_gyro_y.txt',\n",
       " 'body_gyro_z.txt',\n",
       " 'total_acc_x.txt',\n",
       " 'total_acc_y.txt',\n",
       " 'total_acc_z.txt',\n",
       " 'y_labels.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Trainings Data\n",
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/data_labeled_training/Inertial Signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Function for loading the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix):\n",
    "    # load data and labels\n",
    "    X, y = load_dataset_group(prefix)\n",
    "    \n",
    "    # zero-offset class values\n",
    "    y = y - 1\n",
    "    \n",
    "    # one hot encode y\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    # return dataset\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a dataset group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group):\n",
    "    \n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    \n",
    "    # total acceleration\n",
    "    filenames += ['/Inertial Signals/total_acc_x.txt',\n",
    "                  '/Inertial Signals/total_acc_y.txt',\n",
    "                  '/Inertial Signals/total_acc_z.txt']\n",
    "    \n",
    "    # body acceleration\n",
    "    filenames += ['/Inertial Signals/body_acc_x.txt',\n",
    "                  '/Inertial Signals/body_acc_y.txt',\n",
    "                  '/Inertial Signals/body_acc_z.txt']\n",
    "    \n",
    "    # body gyroscope\n",
    "    filenames += ['/Inertial Signals/body_gyro_x.txt',\n",
    "                  '/Inertial Signals/body_gyro_y.txt',\n",
    "                  '/Inertial Signals/body_gyro_z.txt']\n",
    "    \n",
    "    # load input data\n",
    "    X = load_group(filenames, group)\n",
    "    \n",
    "    # load class output\n",
    "    y = load_file(group+'/Inertial Signals/y_labels.txt')\n",
    "    \n",
    "    # return X and y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, group):\n",
    "    loaded = list()\n",
    "    \n",
    "    for name in filenames:\n",
    "\n",
    "        data = load_file(group+name)\n",
    "        loaded.append(data)\n",
    "    \n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    #dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    \n",
    "    path = '/tmp/tbr/BARMER/DSP/' + filepath\n",
    "     \n",
    "    with client_hdfs.read(path, encoding = 'utf-8') as reader:\n",
    "        dataframe = read_csv(reader, header=None, delim_whitespace=True)\n",
    "        \n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Execute Function-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "trainX, trainy = load_dataset('data_labeled_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "testX, testy = load_dataset('data_labeled_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.012817e+00, -1.232167e-01,  1.029341e-01, ...,  3.019122e-02,\n",
       "         6.601362e-02,  2.285864e-02],\n",
       "       [ 1.022833e+00, -1.268756e-01,  1.056872e-01, ...,  4.371071e-02,\n",
       "         4.269897e-02,  1.031572e-02],\n",
       "       [ 1.022028e+00, -1.240037e-01,  1.021025e-01, ...,  3.568780e-02,\n",
       "         7.485018e-02,  1.324969e-02],\n",
       "       ...,\n",
       "       [ 1.018445e+00, -1.240696e-01,  1.003852e-01, ...,  3.985177e-02,\n",
       "         1.909445e-03, -2.170124e-03],\n",
       "       [ 1.019372e+00, -1.227451e-01,  9.987355e-02, ...,  3.744932e-02,\n",
       "        -7.982483e-05, -5.642633e-03],\n",
       "       [ 1.021171e+00, -1.213260e-01,  9.498741e-02, ...,  2.881781e-02,\n",
       "        -3.771800e-05, -1.446006e-03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are three main signal types in the raw data:\n",
    "- total acceleration\n",
    "- body acceleration\n",
    "- body gyroscope\n",
    "\n",
    "Each has three axes of data. This means that there are a total of __nine variables for each time step__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Further, each serie sof data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps.\n",
    "\n",
    "These windows of data correspond to the windows of engineered features (rows) in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This means that one row of data has (128Ã—9), or 1,152, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2947"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Define and train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def train_model(trainX, trainy):\n",
    "     \n",
    "    # define parameters\n",
    "    verbose = 1\n",
    "    epochs = 4\n",
    "    batch_size = 64\n",
    "    n_outputs = 6 # number of classes    \n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features\n",
    "    samples = trainX.shape[0]\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(time_steps, rows, columns, channels)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    trainX = trainX.reshape((samples, time_steps, rows, columns, channels))\n",
    "        \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:125: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  tensor_proto.float_val.extend([np.asscalar(x) for x in proto_values])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.5942 - accuracy: 0.7632\n",
      "Epoch 2/4\n",
      "7352/7352 [==============================] - 12s 2ms/step - loss: 0.2284 - accuracy: 0.9142\n",
      "Epoch 3/4\n",
      "7352/7352 [==============================] - 12s 2ms/step - loss: 0.1724 - accuracy: 0.9335\n",
      "Epoch 4/4\n",
      "7352/7352 [==============================] - 12s 2ms/step - loss: 0.1509 - accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "model = train_model(trainX, trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# prepare Data\n",
    "samples = 1\n",
    "time_steps = 4\n",
    "rows = 1\n",
    "columns = 32\n",
    "channels = 9 #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "sample = trainX[:1].reshape((samples, time_steps, rows, columns, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2390742e-03, 2.0718976e-04, 1.8553269e-06, 1.1680596e-01,\n",
       "        8.8174397e-01, 1.9487952e-06]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(testX, testy):\n",
    "\n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    samples = testX.shape[0]\n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features    \n",
    "    testX = testX.reshape((samples, time_steps, rows, columns, channels))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(testX, testy, verbose=0)\n",
    "    return loss, accuracy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate_performance(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5271437393860651, 0.8747879266738892)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Store Model on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_1\", \"layers\": [{\"class_name\": \"ConvLSTM2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "model_json[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Write to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_structure.json\"\n",
    "with client_hdfs.write(path, encoding = 'utf-8', overwrite=True) as writer:\n",
    "    writer.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlflow', 'model_structure.json']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as local H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Upload File to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_weights.h5\"\n",
    "_ = client_hdfs.upload(hdfs_path=path, local_path=\"model_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlflow', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete temp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update MLflow on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mlruns'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_path = mlflow.get_tracking_uri()\n",
    "mlflow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_experiment = mlflow.create_experiment(name='Activity_Recognition')\n",
    "mlflow_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(experiment_id=mlflow_experiment, run_name='01_model_development')\n",
    "mlflow.set_tag(\"tag\", \"development\")\n",
    "mlflow.set_tag(\"tag\", \"evaluation\")\n",
    "mlflow.log_metric(\"loss\", loss)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_param(\"model\",\"Keras\")\n",
    "mlflow.log_artifact(\"01_model_development.ipynb\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mlruns'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = mlflow.get_tracking_uri()\n",
    "local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put a file in ever folder. No empty folders in mlruns or HDFS will not upload them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/mlruns/\"\n",
    "_ = client_hdfs.upload(hdfs_path=path, local_path=local_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/mlruns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete temp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(local_path, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
