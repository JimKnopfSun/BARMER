{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"' \n",
    "     'frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Database built from the recordings of 30 subjects performing activities of daily living while carrying a waist-mounted smartphone with embedded inertial sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Project: https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Connect to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client_hdfs = InsecureClient('http://awscdh6-ma.sap.local:9870', user='dr.who')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_labeled_performance',\n",
       " 'data_labeled_training',\n",
       " 'data_unlabeled_predictions',\n",
       " 'model']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Data from Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body_acc_x.txt',\n",
       " 'body_acc_y.txt',\n",
       " 'body_acc_z.txt',\n",
       " 'body_gyro_x.txt',\n",
       " 'body_gyro_y.txt',\n",
       " 'body_gyro_z.txt',\n",
       " 'total_acc_x.txt',\n",
       " 'total_acc_y.txt',\n",
       " 'total_acc_z.txt',\n",
       " 'y_labels.txt']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Trainings Data\n",
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/data_labeled_training/Inertial Signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Function for loading the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix):\n",
    "    # load data and labels\n",
    "    X, y = load_dataset_group(prefix)\n",
    "    \n",
    "    # zero-offset class values\n",
    "    y = y - 1\n",
    "    \n",
    "    # one hot encode y\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    # return dataset\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a dataset group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group):\n",
    "    \n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    \n",
    "    # total acceleration\n",
    "    filenames += ['/Inertial Signals/total_acc_x.txt',\n",
    "                  '/Inertial Signals/total_acc_y.txt',\n",
    "                  '/Inertial Signals/total_acc_z.txt']\n",
    "    \n",
    "    # body acceleration\n",
    "    filenames += ['/Inertial Signals/body_acc_x.txt',\n",
    "                  '/Inertial Signals/body_acc_y.txt',\n",
    "                  '/Inertial Signals/body_acc_z.txt']\n",
    "    \n",
    "    # body gyroscope\n",
    "    filenames += ['/Inertial Signals/body_gyro_x.txt',\n",
    "                  '/Inertial Signals/body_gyro_y.txt',\n",
    "                  '/Inertial Signals/body_gyro_z.txt']\n",
    "    \n",
    "    # load input data\n",
    "    X = load_group(filenames, group)\n",
    "    \n",
    "    # load class output\n",
    "    y = load_file(group+'/Inertial Signals/y_labels.txt')\n",
    "    \n",
    "    # return X and y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, group):\n",
    "    loaded = list()\n",
    "    \n",
    "    for name in filenames:\n",
    "\n",
    "        data = load_file(group+name)\n",
    "        loaded.append(data)\n",
    "    \n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    #dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    \n",
    "    path = '/tmp/tbr/BARMER/DSP/' + filepath\n",
    "     \n",
    "    with client_hdfs.read(path, encoding = 'utf-8') as reader:\n",
    "        dataframe = read_csv(reader, header=None, delim_whitespace=True)\n",
    "        \n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Execute Function-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "trainX, trainy = load_dataset('data_labeled_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "testX, testy = load_dataset('data_labeled_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.012817e+00, -1.232167e-01,  1.029341e-01, ...,  3.019122e-02,\n",
       "         6.601362e-02,  2.285864e-02],\n",
       "       [ 1.022833e+00, -1.268756e-01,  1.056872e-01, ...,  4.371071e-02,\n",
       "         4.269897e-02,  1.031572e-02],\n",
       "       [ 1.022028e+00, -1.240037e-01,  1.021025e-01, ...,  3.568780e-02,\n",
       "         7.485018e-02,  1.324969e-02],\n",
       "       ...,\n",
       "       [ 1.018445e+00, -1.240696e-01,  1.003852e-01, ...,  3.985177e-02,\n",
       "         1.909445e-03, -2.170124e-03],\n",
       "       [ 1.019372e+00, -1.227451e-01,  9.987355e-02, ...,  3.744932e-02,\n",
       "        -7.982483e-05, -5.642633e-03],\n",
       "       [ 1.021171e+00, -1.213260e-01,  9.498741e-02, ...,  2.881781e-02,\n",
       "        -3.771800e-05, -1.446006e-03]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are three main signal types in the raw data:\n",
    "- total acceleration\n",
    "- body acceleration\n",
    "- body gyroscope\n",
    "\n",
    "Each has three axes of data. This means that there are a total of __nine variables for each time step__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Further, each serie sof data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps.\n",
    "\n",
    "These windows of data correspond to the windows of engineered features (rows) in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This means that one row of data has (128Ã—9), or 1,152, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2947"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Define and train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def train_model(trainX, trainy):\n",
    "     \n",
    "    # define parameters\n",
    "    verbose = 1\n",
    "    epochs = 25\n",
    "    batch_size = 64\n",
    "    n_outputs = 6 # number of classes    \n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features\n",
    "    samples = trainX.shape[0]\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(time_steps, rows, columns, channels)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    trainX = trainX.reshape((samples, time_steps, rows, columns, channels))\n",
    "        \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 16s 2ms/step - loss: 0.5881 - accuracy: 0.7693\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.2070 - accuracy: 0.9177\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.1464 - accuracy: 0.9410\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.1398 - accuracy: 0.9416\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1182 - accuracy: 0.9498\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1258 - accuracy: 0.9486\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1075 - accuracy: 0.9539\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1105 - accuracy: 0.9538\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.1041 - accuracy: 0.9554\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1013 - accuracy: 0.9557\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1175 - accuracy: 0.9501\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.1162 - accuracy: 0.9512\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 0.0944 - accuracy: 0.9585\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0930 - accuracy: 0.9592\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0991 - accuracy: 0.9573\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0896 - accuracy: 0.9585\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0817 - accuracy: 0.9634\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0892 - accuracy: 0.9601\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0800 - accuracy: 0.9640\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0772 - accuracy: 0.9642\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 16s 2ms/step - loss: 0.0726 - accuracy: 0.9640\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 17s 2ms/step - loss: 0.0707 - accuracy: 0.9642\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 16s 2ms/step - loss: 0.0674 - accuracy: 0.9663\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0739 - accuracy: 0.9652\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 15s 2ms/step - loss: 0.0904 - accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "model = train_model(trainX, trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# prepare Data\n",
    "samples = 1\n",
    "time_steps = 4\n",
    "rows = 1\n",
    "columns = 32\n",
    "channels = 9 #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "sample = trainX[:1].reshape((samples, time_steps, rows, columns, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.0313435e-07, 4.7628955e-06, 1.9645756e-07, 3.4522134e-04,\n",
       "        9.9964929e-01, 1.3913454e-07]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(testX, testy):\n",
    "\n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    samples = testX.shape[0]\n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features    \n",
    "    testX = testX.reshape((samples, time_steps, rows, columns, channels))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(testX, testy, verbose=0)\n",
    "    return loss, accuracy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate_performance(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6291025290808229, 0.9022734761238098)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Store Performance on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Create Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "performance = {\n",
    "    \"timestamp\":[datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")],\n",
    "    \"accuracy\":[accuracy]              \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': ['27/01/2020 11:04:55'], 'accuracy': [0.9022734761238098]}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Write to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_performance.csv\"\n",
    "\n",
    "with client_hdfs.write(path, encoding = 'utf-8', overwrite=True) as writer:\n",
    "    pd.DataFrame(performance).to_csv(writer, mode='a', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_performance.csv', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Store Model on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_2\", \"layers\": [{\"class_name\": \"ConvLSTM2'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "model_json[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Write to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_structure.json\"\n",
    "with client_hdfs.write(path, encoding = 'utf-8', overwrite=True) as writer:\n",
    "    writer.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_performance.csv', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as local H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Upload File to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_weights.h5\"\n",
    "_ = client_hdfs.upload(hdfs_path=path, local_path=\"model_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_performance.csv', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
