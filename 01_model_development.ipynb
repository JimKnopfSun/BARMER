{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XOEN9W05_4A\"' \n",
    "     'frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Database built from the recordings of 30 subjects performing activities of daily living while carrying a waist-mounted smartphone with embedded inertial sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Project: https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data: https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Connect to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client_hdfs = InsecureClient('http://awscdh6-ma.sap.local:9870', user='dr.who')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_labeled_performance',\n",
       " 'data_labeled_training',\n",
       " 'data_unlabeled_predictions',\n",
       " 'model']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Data (labeled) from Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body_acc_x.txt',\n",
       " 'body_acc_y.txt',\n",
       " 'body_acc_z.txt',\n",
       " 'body_gyro_x.txt',\n",
       " 'body_gyro_y.txt',\n",
       " 'body_gyro_z.txt',\n",
       " 'total_acc_x.txt',\n",
       " 'total_acc_y.txt',\n",
       " 'total_acc_z.txt',\n",
       " 'y_labels.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Trainings Data\n",
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/data_labeled_training/Inertial Signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Function for loading the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix):\n",
    "    # load data and labels\n",
    "    X, y = load_dataset_group(prefix)\n",
    "    \n",
    "    # zero-offset class values\n",
    "    y = y - 1\n",
    "    \n",
    "    # one hot encode y\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    # return dataset\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a dataset group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group):\n",
    "    \n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    \n",
    "    # total acceleration\n",
    "    filenames += ['/Inertial Signals/total_acc_x.txt',\n",
    "                  '/Inertial Signals/total_acc_y.txt',\n",
    "                  '/Inertial Signals/total_acc_z.txt']\n",
    "    \n",
    "    # body acceleration\n",
    "    filenames += ['/Inertial Signals/body_acc_x.txt',\n",
    "                  '/Inertial Signals/body_acc_y.txt',\n",
    "                  '/Inertial Signals/body_acc_z.txt']\n",
    "    \n",
    "    # body gyroscope\n",
    "    filenames += ['/Inertial Signals/body_gyro_x.txt',\n",
    "                  '/Inertial Signals/body_gyro_y.txt',\n",
    "                  '/Inertial Signals/body_gyro_z.txt']\n",
    "    \n",
    "    # load input data\n",
    "    X = load_group(filenames, group)\n",
    "    \n",
    "    # load class output\n",
    "    y = load_file(group+'/Inertial Signals/y_labels.txt')\n",
    "    \n",
    "    # return X and y\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a group of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, group):\n",
    "    loaded = list()\n",
    "    \n",
    "    for name in filenames:\n",
    "\n",
    "        data = load_file(group+name)\n",
    "        loaded.append(data)\n",
    "    \n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A function for loading a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    #dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    \n",
    "    path = '/tmp/tbr/BARMER/DSP/' + filepath\n",
    "     \n",
    "    with client_hdfs.read(path, encoding = 'utf-8') as reader:\n",
    "        dataframe = read_csv(reader, header=None, delim_whitespace=True)\n",
    "        \n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Execute Function-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "trainX, trainy = load_dataset('data_labeled_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "testX, testy = load_dataset('data_labeled_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.012817e+00, -1.232167e-01,  1.029341e-01, ...,  3.019122e-02,\n",
       "         6.601362e-02,  2.285864e-02],\n",
       "       [ 1.022833e+00, -1.268756e-01,  1.056872e-01, ...,  4.371071e-02,\n",
       "         4.269897e-02,  1.031572e-02],\n",
       "       [ 1.022028e+00, -1.240037e-01,  1.021025e-01, ...,  3.568780e-02,\n",
       "         7.485018e-02,  1.324969e-02],\n",
       "       ...,\n",
       "       [ 1.018445e+00, -1.240696e-01,  1.003852e-01, ...,  3.985177e-02,\n",
       "         1.909445e-03, -2.170124e-03],\n",
       "       [ 1.019372e+00, -1.227451e-01,  9.987355e-02, ...,  3.744932e-02,\n",
       "        -7.982483e-05, -5.642633e-03],\n",
       "       [ 1.021171e+00, -1.213260e-01,  9.498741e-02, ...,  2.881781e-02,\n",
       "        -3.771800e-05, -1.446006e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are three main signal types in the raw data:\n",
    "- total acceleration\n",
    "- body acceleration\n",
    "- body gyroscope\n",
    "\n",
    "Each has three axes of data. This means that there are a total of __nine variables for each time step__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Further, each serie sof data has been partitioned into overlapping windows of 2.65 seconds of data, or 128 time steps.\n",
    "\n",
    "These windows of data correspond to the windows of engineered features (rows) in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This means that one row of data has (128Ã—9), or 1,152, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2947"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Define and train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def train_model(trainX, trainy):\n",
    "     \n",
    "    # define parameters\n",
    "    verbose = 1\n",
    "    epochs = 2\n",
    "    batch_size = 64\n",
    "    n_outputs = 6 # number of classes    \n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features\n",
    "    samples = trainX.shape[0]\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(time_steps, rows, columns, channels)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    trainX = trainX.reshape((samples, time_steps, rows, columns, channels))\n",
    "        \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\keras\\engine\\training_utils.py:811: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if isinstance(loss, collections.Mapping):\n",
      "C:\\Users\\tbraeutigam\\AppData\\Local\\Continuum\\anaconda3\\envs\\BARMER_DSP\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(values, collections.Sequence):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.6069 - accuracy: 0.7586: 0s - loss: 0.6219 - accu\n",
      "Epoch 2/2\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2201 - accuracy: 0.9155\n"
     ]
    }
   ],
   "source": [
    "model = train_model(trainX, trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# prepare Data\n",
    "samples = 1\n",
    "time_steps = 4\n",
    "rows = 1\n",
    "columns = 32\n",
    "channels = 9 #number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "sample = trainX[:1].reshape((samples, time_steps, rows, columns, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.45889982e-03, 1.18818134e-04, 5.19116884e-06, 7.05881715e-02,\n",
       "        9.27821219e-01, 7.66064386e-06]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 0 WALKING\n",
    "- 1 WALKING_UPSTAIRS\n",
    "- 2 WALKING_DOWNSTAIRS\n",
    "- 3 SITTING\n",
    "- 4 STANDING\n",
    "- 5 LAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate_performance(testX, testy):\n",
    "\n",
    "    # reshape data into subsequences (samples, time steps, rows, cols, channels)\n",
    "    samples = testX.shape[0]\n",
    "    time_steps = 4\n",
    "    rows = 1\n",
    "    columns = 32\n",
    "    channels = 9 #number of features    \n",
    "    testX = testX.reshape((samples, time_steps, rows, columns, channels))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(testX, testy, verbose=0)\n",
    "    return loss, accuracy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate_performance(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43640102741498443, 0.8710553050041199)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Performance on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7edf7694857f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "mlflow.start_run()\n",
    "mlflow.log_metric(\"loss\", loss)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tbr/BARMER/DSP/model/model_performance.csv'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = mlflow.get_tracking_uri()\n",
    "local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/mlflow/\"\n",
    "_ = client_hdfs.upload(hdfs_path=path, local_path=local_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/mlflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Store Model on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_2\", \"layers\": [{\"class_name\": \"ConvLSTM2'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "model_json[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Write to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_structure.json\"\n",
    "with client_hdfs.write(path, encoding = 'utf-8', overwrite=True) as writer:\n",
    "    writer.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_performance.csv', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Serialize as local H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Upload File to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/tbr/BARMER/DSP/model/model_weights.h5\"\n",
    "_ = client_hdfs.upload(hdfs_path=path, local_path=\"model_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_performance.csv', 'model_structure.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.list('/tmp/tbr/BARMER/DSP/model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
